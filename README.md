# Group No: 73
**Group Member Names:**
* DEEPESH PARMAR (2024AA05053) 100%
* PRAJAPATI HEMANG KEYURBHAI (2024AA05058) 100%
* SATISH KUMAR PATHAK (2024AA05578) 100%
* V. NIKITHA (2024AA05552) 100%
* VENKATARAMANAN S (2024AA05555) 100%

# üìÑ Document-Based Question Answering System

A robust, Retrieval-Augmented Generation (RAG) application that allows users to chat with their documents (PDF, DOCX, TXT). 

Built with **FastAPI** (Backend) and **Streamlit** (Frontend), this system leverages **Google Gemini Flash** for reasoning and **HuggingFace Local Embeddings** for vector search and embeddings.

---

## üöÄ Features

* **Multi-Document Support:** Upload and index multiple PDF, DOCX, or TXT files simultaneously.
* **Tech Stack:**
    * **LLM:** Google Gemini Flash (via Google AI Studio).
    * **Embeddings:** HuggingFace `all-MiniLM-L6-v2` (Running locally on CPU).
* **Interactive Chat UI:** Streamlit-based chat interface supporting follow-up questions.
* **Real-Time Confidence Scoring:** displays the "Confidence Score" (vector similarity) for the top source.
* **Source Citation:** Accurate referencing of the exact document source and excerpt used to generate the answer.
* **Decoupled Architecture:** Separate Frontend (Streamlit) and Backend (FastAPI) for scalability.

---

## üõ†Ô∏è Tech Stack

* **Backend:** FastAPI, Uvicorn
* **Frontend:** Streamlit
* **Orchestration:** LangChain
* **Vector Database:** ChromaDB (Persistent Local Storage)
* **AI Models:** * *Generation:* Google Gemini Flash
    * *Embedding:* Sentence-Transformers (HuggingFace)

---

## üìÇ Project Structure

```text
doc_qa_app/
‚îú‚îÄ‚îÄ backend.py           # FastAPI server (Logic & API endpoints)
‚îú‚îÄ‚îÄ frontend.py          # Streamlit UI (Chat interface)
‚îú‚îÄ‚îÄ requirements.txt     # Python dependencies
‚îú‚îÄ‚îÄ .env                 # API Keys (Not committed to repo)
‚îú‚îÄ‚îÄ chroma_db_free/      # Local Vector Store (Auto-generated)
‚îî‚îÄ‚îÄ uploads/             # Temp storage for uploaded files
```

## ‚öôÔ∏è Installation & Setup

**Prerequisites**
* Python 3.9 or higher
* A Google Account (to get the free API key)

**Step 1: Clone or Extract the Project Folder:**
```bash
cd document-based-qa-system
```

**Step 2: Create a Virtual Environment (Optional but Recommended):**
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Mac/Linux
source venv/bin/activate
```

**Step 3: Install Dependencies:**
```bash
pip install -r requirements.txt
```

**Step 4: Set Up Environment Variables**
* Create a file named .env in the root directory and add your Google Gemini API Key.
* Get a key from Google AI Studio.
* Add it to .env
```bash
GOOGLE_API_KEY=<YOUR_API_KEY>
```

**Step 5: Run the Application:**

**Terminal 1: Start the Backend API**
```bash
python backend.py
```

**Terminal 2: Start the Frontend UI**
```bash
streamlit run frontend.py
```

## üìñ Usage Guide

1. Build Knowledge Base:
    * Open the sidebar on the left.
    * Click "Browse files" to select PDF or DOCX documents.
    * Click "Update Knowledge Base".
    * Wait for the success message (this builds the vector index locally).

2. Ask Questions:
    * Type your question in the chat input box (e.g., "What are the safety protocols?").
    * The AI will answer based only on your documents.

3. View Sources:
    * Expand the "üîç Top Source" box below the answer to see the exact text chunk and the confidence score (Green/Orange/Red based on relevance).